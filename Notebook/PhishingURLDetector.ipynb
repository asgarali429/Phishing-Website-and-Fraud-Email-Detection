{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-V16NhyTzkb",
        "outputId": "a23f5641-2705-408d-fd0e-cf0940909d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tldextract\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2024.12.14)\n",
            "Downloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-2.1.0 tldextract-5.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tldextract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from urllib.parse import urlparse\n",
        "import tldextract\n",
        "from gensim.models import Word2Vec\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import ipaddress"
      ],
      "metadata": {
        "id": "4W8ji8unDJvq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1t5vPLOwTwvJ"
      },
      "outputs": [],
      "source": [
        "class URLFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.suspicious_keywords = [\n",
        "            'login', 'verify', 'update', 'account', 'secure', 'banking',\n",
        "            'signin', 'confirm', 'password', 'credential', 'security'\n",
        "        ]\n",
        "        self.suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq', '.xyz']\n",
        "\n",
        "    def extract_features(self, url):\n",
        "        \"\"\"Extract features from a single URL\"\"\"\n",
        "        features = {}\n",
        "        parsed_url = urlparse(url.lower())\n",
        "\n",
        "        # Basic URL Characteristics\n",
        "        features['url_length'] = len(url)\n",
        "        features['domain_length'] = len(parsed_url.netloc)\n",
        "        features['path_length'] = len(parsed_url.path)\n",
        "        features['is_https'] = 1 if parsed_url.scheme == 'https' else 0\n",
        "\n",
        "        # Special characters count\n",
        "        features['special_char_count'] = sum(url.count(c) for c in \"@#$%^&*()+=[]{}|;:,.<>?\")\n",
        "\n",
        "        # Check if domain is an IP address\n",
        "        features['is_ip_address'] = self._is_ip_address(parsed_url.netloc)\n",
        "\n",
        "        # Suspicious Keywords Count\n",
        "        features['suspicious_keyword_count'] = self._count_suspicious_keywords(url)\n",
        "\n",
        "        # Top-level domain check for suspicious TLDs\n",
        "        features['has_suspicious_tld'] = self._check_suspicious_tld(parsed_url.netloc)\n",
        "\n",
        "        # URL structure features\n",
        "        features['directory_depth'] = parsed_url.path.count('/')\n",
        "        features['query_param_count'] = len(parse_qs(parsed_url.query))\n",
        "        features['fragment_length'] = len(parsed_url.fragment)\n",
        "\n",
        "        # Additional features\n",
        "        features['has_subdomain'] = 1 if parsed_url.netloc.count('.') > 1 else 0\n",
        "        features['domain_has_numbers'] = 1 if any(char.isdigit() for char in parsed_url.netloc) else 0\n",
        "        features['query_param_length'] = sum(len(val) for key, val in parse_qs(parsed_url.query).items())\n",
        "        features['subdomain_count'] = parsed_url.netloc.count('.') - 1\n",
        "        features['uses_ip_in_url'] = 1 if parsed_url.netloc.replace('.', '').isdigit() else 0\n",
        "        features['long_path'] = 1 if len(parsed_url.path) > 50 else 0\n",
        "        features['has_hyphens_in_domain'] = 1 if '-' in parsed_url.netloc else 0\n",
        "        features['has_at_symbol'] = 1 if '@' in url else 0\n",
        "\n",
        "        # Synthetic phishing-specific patterns\n",
        "        features['has_misspelled_domain'] = self._has_misspelled_domain(parsed_url.netloc)\n",
        "        features['is_shortened_url'] = self._is_shortened_url(url)\n",
        "\n",
        "        # Domain extraction without status code\n",
        "        features['domain_name'], features['tld'] = self._extract_domain_tld(url)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _is_ip_address(self, domain):\n",
        "        \"\"\"Check if the domain is an IP address\"\"\"\n",
        "        try:\n",
        "            ipaddress.ip_address(domain.split(':')[0])\n",
        "            return 1\n",
        "        except ValueError:\n",
        "            return 0\n",
        "\n",
        "    def _count_suspicious_keywords(self, url):\n",
        "        \"\"\"Count suspicious keywords in the URL\"\"\"\n",
        "        return sum(1 for keyword in self.suspicious_keywords if keyword in url)\n",
        "\n",
        "    def _check_suspicious_tld(self, domain):\n",
        "        \"\"\"Check for suspicious top-level domains\"\"\"\n",
        "        return int(any(domain.endswith(tld) for tld in self.suspicious_tlds))\n",
        "\n",
        "    def _has_misspelled_domain(self, domain):\n",
        "        misspelled_patterns = [\n",
        "            r\"0{1,}o\",\n",
        "            r\"1{1,}l\",\n",
        "            r\"3{1,}e\",\n",
        "            r\"1{1,}i\",\n",
        "            r\"(.)\\1{2,}\",\n",
        "            r\"faecbook\",\n",
        "            r\"gogle\",\n",
        "            r\"(o{2,}|g{2,}|e{2,})\",\n",
        "            r\"ht{1,}:\\/\\/\",\n",
        "            r\"g{2,}le\",\n",
        "            r\"fa{2,}cebook\",\n",
        "            r\"0o{1,}gle\",\n",
        "            r\"t{1,}witter\",\n",
        "        ]\n",
        "\n",
        "        return int(any(re.search(pattern, domain) for pattern in misspelled_patterns))\n",
        "\n",
        "    def _is_shortened_url(self, url):\n",
        "        \"\"\"Check if the URL is a shortened link\"\"\"\n",
        "        shortened_domains = [\n",
        "            'bit.ly', 'tinyurl.com', 'goo.gl', 't.co', 'ow.ly', 'is.gd', 'buff.ly', 'adf.ly'\n",
        "        ]\n",
        "        domain = urlparse(url).netloc\n",
        "        return int(domain in shortened_domains)\n",
        "\n",
        "    def _extract_domain_tld(self, url):\n",
        "        \"\"\"Extract domain name and TLD\"\"\"\n",
        "        extracted = tldextract.extract(url)\n",
        "        domain_name = extracted.domain\n",
        "        tld = extracted.suffix\n",
        "        return domain_name, tld"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PhishingURLDetector:\n",
        "    def __init__(self):\n",
        "        self.word2vec_model = None\n",
        "        self.classifier = None\n",
        "        self.label_encoder = None\n",
        "        self.feature_extractor = URLFeatureExtractor()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.label_map = {0: 'phishing', 1: 'safe'}  # Fixed label mapping to match your data\n",
        "        self.vector_size = 100\n",
        "        self.tld_encoder = LabelEncoder()  # Separate encoder for TLDs\n",
        "\n",
        "    def save_models(self, model_path='phishing_detector.joblib'):\n",
        "        \"\"\"Save all trained models and encoders\"\"\"\n",
        "        if not all([self.word2vec_model, self.classifier, self.label_encoder, self.tld_encoder]):\n",
        "            raise ValueError(\"Models not trained! Call train() first.\")\n",
        "\n",
        "        models = {\n",
        "            'word2vec': self.word2vec_model,\n",
        "            'classifier': self.classifier,\n",
        "            'label_encoder': self.label_encoder,\n",
        "            'tld_encoder': self.tld_encoder\n",
        "        }\n",
        "        joblib.dump(models, model_path)\n",
        "\n",
        "    def load_models(self, model_path='phishing_detector.joblib'):\n",
        "        \"\"\"Load all trained models and encoders\"\"\"\n",
        "        try:\n",
        "            models = joblib.load(model_path)\n",
        "            self.word2vec_model = models['word2vec']\n",
        "            self.classifier = models['classifier']\n",
        "            self.label_encoder = models['label_encoder']\n",
        "            self.tld_encoder = models['tld_encoder']\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error loading models: {str(e)}\")\n",
        "\n",
        "    def train(self, urls, labels):\n",
        "        \"\"\"Train the complete model pipeline\"\"\"\n",
        "        if not isinstance(urls, (pd.Series, list)) or not isinstance(labels, (pd.Series, list, np.ndarray)):\n",
        "            raise ValueError(\"URLs and labels must be pandas Series, lists, or numpy arrays\")\n",
        "\n",
        "        # Convert inputs to numpy arrays\n",
        "        urls = np.array(urls)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # Fit label encoder\n",
        "        self.label_encoder.fit(labels)\n",
        "        labels_encoded = self.label_encoder.transform(labels)\n",
        "\n",
        "        # Train Word2Vec model on domains\n",
        "        tokenized_domains = [list(str(tldextract.extract(url).domain).lower())\n",
        "                           for url in urls]\n",
        "        self.word2vec_model = Word2Vec(\n",
        "            sentences=tokenized_domains,\n",
        "            vector_size=self.vector_size,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            workers=4\n",
        "        )\n",
        "\n",
        "        # Extract and encode TLDs\n",
        "        tlds = [tldextract.extract(url).suffix for url in urls]\n",
        "        self.tld_encoder.fit(tlds)\n",
        "\n",
        "        # Prepare features\n",
        "        X = self._prepare_features(urls)\n",
        "\n",
        "        # Train stacking classifier\n",
        "        base_models = [\n",
        "            ('rf', RandomForestClassifier(\n",
        "                n_estimators=183,\n",
        "                max_depth=21,\n",
        "                min_samples_split=7,\n",
        "                min_samples_leaf=4,\n",
        "                max_features='sqrt',\n",
        "                random_state=42\n",
        "            )),\n",
        "            ('gb', GradientBoostingClassifier(\n",
        "                n_estimators=50,\n",
        "                random_state=42\n",
        "            ))\n",
        "        ]\n",
        "\n",
        "        meta_model = LogisticRegression(random_state=42)\n",
        "        self.classifier = StackingClassifier(\n",
        "            estimators=base_models,\n",
        "            final_estimator=meta_model,\n",
        "            cv=3\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            self.classifier.fit(X, labels_encoded)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error training classifier: {str(e)}\")\n",
        "\n",
        "    def predict(self, url):\n",
        "        \"\"\"Predict whether a single URL is phishing or safe\"\"\"\n",
        "        if not all([self.word2vec_model, self.classifier, self.label_encoder, self.tld_encoder]):\n",
        "            raise ValueError(\"Models not trained! Call train() or load_models() first.\")\n",
        "\n",
        "        try:\n",
        "            # Prepare features for the single URL\n",
        "            features = self._prepare_single_url_features(url)\n",
        "\n",
        "            # Make prediction\n",
        "            prediction = self.classifier.predict(features)[0]\n",
        "            probability = self.classifier.predict_proba(features)[0]\n",
        "\n",
        "            # Map prediction to label\n",
        "            label = self.label_map.get(prediction, 'Unknown')\n",
        "\n",
        "            # Note: probability[0] is phishing probability and probability[1] is safe probability\n",
        "            # because 0 means phishing and 1 means safe in your data\n",
        "            return {\n",
        "                'url': url,\n",
        "                'prediction': label,\n",
        "                'is_phishing': label == 'phishing',\n",
        "                'confidence': max(probability),\n",
        "                'probability_phishing': probability[0],  # Fixed probability mapping\n",
        "                'probability_safe': probability[1]       # Fixed probability mapping\n",
        "            }\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error making prediction: {str(e)}\")\n",
        "\n",
        "    def _prepare_single_url_features(self, url):\n",
        "        \"\"\"Prepare features for a single URL\"\"\"\n",
        "        try:\n",
        "            # Extract base features\n",
        "            features = self.feature_extractor.extract_features(url)\n",
        "\n",
        "            # Get domain embedding\n",
        "            domain = str(features['domain_name']).lower()\n",
        "            domain_tokens = list(domain)\n",
        "\n",
        "            # Calculate domain embedding\n",
        "            valid_tokens = [token for token in domain_tokens if token in self.word2vec_model.wv]\n",
        "            if valid_tokens:\n",
        "                domain_embedding = np.mean([self.word2vec_model.wv[token] for token in valid_tokens], axis=0)\n",
        "            else:\n",
        "                domain_embedding = np.zeros(self.vector_size)\n",
        "\n",
        "            # Prepare feature dictionary\n",
        "            feature_dict = {\n",
        "                'url_length': features['url_length'],\n",
        "                'domain_length': features['domain_length'],\n",
        "                'path_length': features['path_length'],\n",
        "                'is_https': features['is_https'],\n",
        "                'special_char_count': features['special_char_count'],\n",
        "                'is_ip_address': features['is_ip_address'],\n",
        "                'suspicious_keyword_count': features['suspicious_keyword_count'],\n",
        "                'has_suspicious_tld': features['has_suspicious_tld'],\n",
        "                'directory_depth': features['directory_depth'],\n",
        "                'query_param_count': features['query_param_count'],\n",
        "                'fragment_length': features['fragment_length'],\n",
        "                'has_subdomain': features['has_subdomain'],\n",
        "                'domain_has_numbers': features['domain_has_numbers'],\n",
        "                'query_param_length': features['query_param_length'],\n",
        "                'subdomain_count': features['subdomain_count'],\n",
        "                'uses_ip_in_url': features['uses_ip_in_url'],\n",
        "                'long_path': features['long_path'],\n",
        "                'has_hyphens_in_domain': features['has_hyphens_in_domain'],\n",
        "                'has_at_symbol': features['has_at_symbol'],\n",
        "                'has_misspelled_domain': features['has_misspelled_domain'],\n",
        "                'is_shortened_url': features['is_shortened_url']\n",
        "            }\n",
        "\n",
        "            # Add TLD encoding\n",
        "            feature_dict['tld_encoded'] = self._encode_tld(features['tld'])\n",
        "\n",
        "            # Add embedding features\n",
        "            for i, val in enumerate(domain_embedding):\n",
        "                feature_dict[f'embedding_{i}'] = val\n",
        "\n",
        "            return pd.DataFrame([feature_dict])\n",
        "\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error preparing features: {str(e)}\")\n",
        "\n",
        "    def _encode_tld(self, tld):\n",
        "        \"\"\"Encode TLD, handling unknown values\"\"\"\n",
        "        try:\n",
        "            return self.tld_encoder.transform([str(tld)])[0]\n",
        "        except ValueError:\n",
        "            return -1\n",
        "\n",
        "    def _prepare_features(self, urls):\n",
        "        \"\"\"Prepare features for a list of URLs\"\"\"\n",
        "        features_list = []\n",
        "        for url in urls:\n",
        "            try:\n",
        "                features = self._prepare_single_url_features(url)\n",
        "                features_list.append(features)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error processing URL {url}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if not features_list:\n",
        "            raise ValueError(\"No valid features could be extracted from URLs\")\n",
        "\n",
        "        return pd.concat(features_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "i-PQ4OeiApdU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HDlogMjawP8g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67fb54f8-c249-4dc3-c9c8-792204888267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (235810, 2)\n",
            "\n",
            "Sample of the data:\n",
            "                                  URL  label\n",
            "0    https://www.southbankmosaics.com      1\n",
            "1            https://www.uni-mainz.de      1\n",
            "2      https://www.voicefmradio.co.uk      1\n",
            "3         https://www.sfnmjournal.com      1\n",
            "4  https://www.rewildingargentina.org      1\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('url.csv')  # Loading Dataset\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"\\nSample of the data:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7xp06nt0yMJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9ade50-eeeb-45e4-f205-f259d3a90e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 8000\n",
            "Testing samples: 2000\n"
          ]
        }
      ],
      "source": [
        "df_subset = df.iloc[-10000:].reset_index(drop=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_subset['URL'],\n",
        "    df_subset['label'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I744WJ8zyrZT"
      },
      "outputs": [],
      "source": [
        "# Initialize the detector\n",
        "detector = PhishingURLDetector()\n",
        "\n",
        "# Train using your existing dataset\n",
        "detector.train(df['URL'], df['label'])\n",
        "\n",
        "# Save the trained models\n",
        "detector.save_models('phishing_detector.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eighth cell - Test the model\n",
        "# Load the model (optional if you've just trained it)\n",
        "detector = PhishingURLDetector()\n",
        "detector.load_models('phishing_detector.joblib')"
      ],
      "metadata": {
        "id": "t09bRIWsD9bK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test URLs\n",
        "test_urls = [\n",
        "    \"https://www.google.com\",\n",
        "    \"https://facebook.com\",\n",
        "    \"http://suspicious-login.tk/verify\",\n",
        "    \"https://www.go0gle.com/\"\n",
        "]\n",
        "\n",
        "print(\"Testing URLs:\")\n",
        "for url in test_urls:\n",
        "    result = detector.predict(url)\n",
        "    print(f\"\\nURL: {result['url']}\")\n",
        "    print(f\"Prediction: {result['prediction']}\")\n",
        "    print(f\"Confidence: {result['confidence']:.2f}\")\n",
        "    print(f\"Probability of being phishing: {result['probability_phishing']:.2f}\")\n",
        "    print(f\"Probability of being safe: {result['probability_safe']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN6yA-qgCbMa",
        "outputId": "ca3d3443-8383-4429-8b5c-d1aa5bfca439"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing URLs:\n",
            "\n",
            "URL: https://www.google.com\n",
            "Prediction: safe\n",
            "Confidence: 0.99\n",
            "Probability of being phishing: 0.01\n",
            "Probability of being safe: 0.99\n",
            "\n",
            "URL: https://facebook.com\n",
            "Prediction: phishing\n",
            "Confidence: 0.97\n",
            "Probability of being phishing: 0.97\n",
            "Probability of being safe: 0.03\n",
            "\n",
            "URL: http://suspicious-login.tk/verify\n",
            "Prediction: phishing\n",
            "Confidence: 1.00\n",
            "Probability of being phishing: 1.00\n",
            "Probability of being safe: 0.00\n",
            "\n",
            "URL: https://www.go0gle.com/\n",
            "Prediction: phishing\n",
            "Confidence: 1.00\n",
            "Probability of being phishing: 1.00\n",
            "Probability of being safe: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = []\n",
        "for url in X_test:\n",
        "    result = detector.predict(url)\n",
        "    y_pred.append(1 if result['prediction'] == 'safe' else 0)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3jGgEg8EUP0",
        "outputId": "79bade18-18df-4dc3-f5c1-3fe60a81092b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       853\n",
            "           1       1.00      1.00      1.00      1147\n",
            "\n",
            "    accuracy                           1.00      2000\n",
            "   macro avg       1.00      1.00      1.00      2000\n",
            "weighted avg       1.00      1.00      1.00      2000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 850    3]\n",
            " [   0 1147]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D4SWmaQqEkqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}